{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a basic ANN on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "import torch  # https://pytorch.org\n",
    "import torchvision  # https://pytorch.org\n",
    "from jaxtyping import Array, Float, Int, PyTree  # https://github.com/google/jaxtyping\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "LEARNING_RATE = 0.005\n",
    "\n",
    "STEPS = 300\n",
    "\n",
    "PRINT_EVERY = 30\n",
    "\n",
    "SEED = 5678\n",
    "\n",
    "key = jax.random.PRNGKey(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalise_data = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=normalise_data,\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=normalise_data,\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 28, 28)\n",
      "(64,)\n",
      "[1 7 5 0 8 1 4 2 7 9 8 3 9 2 3 4 5 5 7 9 2 1 0 6 9 5 4 4 2 9 3 9 9 0 9 3 7\n",
      " 4 7 2 7 8 6 1 6 2 9 4 5 3 9 1 5 1 8 5 1 4 8 0 9 4 0 6]\n"
     ]
    }
   ],
   "source": [
    "# Checking our data a bit (by now, everyone knows what the MNIST dataset looks like)\n",
    "dummy_x, dummy_y = next(iter(trainloader))\n",
    "\n",
    "dummy_x = dummy_x.numpy()\n",
    "dummy_y = dummy_y.numpy()\n",
    "print(dummy_x.shape)  # 64x1x28x28\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(eqx.Module):\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, key):\n",
    "        key1, key2, key3, key4 = jax.random.split(key, 4)\n",
    "        # Standard CNN setup: convolutional layer, followed by flattening,\n",
    "        # with a small MLP on top.\n",
    "        self.layers = [\n",
    "            eqx.nn.Conv2d(1, 3, kernel_size=4, key=key1),\n",
    "            eqx.nn.MaxPool2d(kernel_size=2),\n",
    "            jax.nn.relu,\n",
    "            jnp.ravel,\n",
    "            eqx.nn.Linear(1728, 512, key=key2),\n",
    "            jax.nn.sigmoid,\n",
    "            eqx.nn.Linear(512, 64, key=key3),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.Linear(64, 10, key=key4),\n",
    "            jax.nn.log_softmax,\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x: Float[Array, \"1 28 28\"]) -> Float[Array, \"10\"]:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "model = CNN(subkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dummy_x[0, 0, 0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  layers=[\n",
      "    Conv2d(\n",
      "      num_spatial_dims=2,\n",
      "      weight=f32[3,1,4,4],\n",
      "      bias=f32[3,1,1],\n",
      "      in_channels=1,\n",
      "      out_channels=3,\n",
      "      kernel_size=(4, 4),\n",
      "      stride=(1, 1),\n",
      "      padding=((0, 0), (0, 0)),\n",
      "      dilation=(1, 1),\n",
      "      groups=1,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    MaxPool2d(\n",
      "      init=-inf,\n",
      "      operation=<function max>,\n",
      "      num_spatial_dims=2,\n",
      "      kernel_size=(2, 2),\n",
      "      stride=(1, 1),\n",
      "      padding=((0, 0), (0, 0)),\n",
      "      use_ceil=False\n",
      "    ),\n",
      "    <wrapped function relu>,\n",
      "    <wrapped function ravel>,\n",
      "    Linear(\n",
      "      weight=f32[512,1728],\n",
      "      bias=f32[512],\n",
      "      in_features=1728,\n",
      "      out_features=512,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    <wrapped function sigmoid>,\n",
      "    Linear(\n",
      "      weight=f32[64,512],\n",
      "      bias=f32[64],\n",
      "      in_features=512,\n",
      "      out_features=64,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    <wrapped function relu>,\n",
      "    Linear(\n",
      "      weight=f32[10,64],\n",
      "      bias=f32[10],\n",
      "      in_features=64,\n",
      "      out_features=10,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    <wrapped function log_softmax>\n",
      "  ]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def loss(\n",
    "    model: CNN, x: Float[Array, \"batch 1 28 28\"], y: Int[Array, \" batch\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    # Our input has the shape (BATCH_SIZE, 1, 28, 28), but our model operations on\n",
    "    # a single input input image of shape (1, 28, 28).\n",
    "    #\n",
    "    # Therefore, we have to use jax.vmap, which in this case maps our model over the\n",
    "    # leading (batch) axis.\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    return cross_entropy(y, pred_y)\n",
    "\n",
    "\n",
    "def cross_entropy(\n",
    "    y: Int[Array, \" batch\"], pred_y: Float[Array, \"batch 10\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    # y are the true targets, and should be integers 0-9.\n",
    "    # pred_y are the log-softmax'd predictions.\n",
    "    pred_y = jnp.take_along_axis(pred_y, jnp.expand_dims(y, 1), axis=1)\n",
    "    return -jnp.mean(pred_y)\n",
    "\n",
    "\n",
    "# Example loss\n",
    "loss_value = loss(model, dummy_x, dummy_y)\n",
    "print(loss_value.shape)  # scalar loss\n",
    "# Example inference\n",
    "output = jax.vmap(model)(dummy_x)\n",
    "print(output.shape)  # batch of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.304172\n"
     ]
    }
   ],
   "source": [
    "# Getting the parameters\n",
    "\n",
    "value, grads = eqx.filter_value_and_grad(loss)(model, dummy_x, dummy_y)\n",
    "print(value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = eqx.filter_jit(loss)  # JIT our loss function from earlier!\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def compute_accuracy(\n",
    "    model: CNN, x: Float[Array, \"batch 1 28 28\"], y: Int[Array, \" batch\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    \"\"\"This function takes as input the current model\n",
    "    and computes the average accuracy on a batch.\n",
    "    \"\"\"\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    pred_y = jnp.argmax(pred_y, axis=1)\n",
    "    return jnp.mean(y == pred_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: CNN, testloader: torch.utils.data.DataLoader):\n",
    "    \"\"\"This function evaluates the model on the test dataset,\n",
    "    computing both the average loss and the average accuracy.\n",
    "    \"\"\"\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    for x, y in testloader:\n",
    "        x = x.numpy()\n",
    "        y = y.numpy()\n",
    "        # Note that all the JAX operations happen inside `loss` and `compute_accuracy`,\n",
    "        # and both have JIT wrappers, so this is fast.\n",
    "        avg_loss += loss(model, x, y)\n",
    "        avg_acc += compute_accuracy(model, x, y)\n",
    "    return avg_loss / len(testloader), avg_acc / len(testloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(2.307958, dtype=float32), Array(0.10081609, dtype=float32))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, testloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: CNN,\n",
    "    trainloader: torch.utils.data.DataLoader,\n",
    "    testloader: torch.utils.data.DataLoader,\n",
    "    optim: optax.GradientTransformation,\n",
    "    steps: int,\n",
    "    print_every: int,\n",
    ") -> CNN:\n",
    "    # Just like earlier: It only makes sense to train the arrays in our model,\n",
    "    # so filter out everything else.\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "    # Always wrap everything -- computing gradients, running the optimiser, updating\n",
    "    # the model -- into a single JIT region. This ensures things run as fast as\n",
    "    # possible.\n",
    "    @eqx.filter_jit\n",
    "    def make_step(\n",
    "        model: CNN,\n",
    "        opt_state: PyTree,\n",
    "        x: Float[Array, \"batch 1 28 28\"],\n",
    "        y: Int[Array, \" batch\"],\n",
    "    ):\n",
    "        loss_value, grads = eqx.filter_value_and_grad(loss)(model, x, y)\n",
    "        updates, opt_state = optim.update(grads, opt_state, model)\n",
    "\n",
    "\n",
    "        print(\"updates = \", updates)\n",
    "        print(\"opt_state =\", opt_state)\n",
    "        \n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, opt_state, loss_value\n",
    "\n",
    "    # Loop over our training dataset as many times as we need.\n",
    "    def infinite_trainloader():\n",
    "        while True:\n",
    "            yield from trainloader\n",
    "\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    for step, (x, y) in zip(range(steps), infinite_trainloader()):\n",
    "        # PyTorch dataloaders give PyTorch tensors by default,\n",
    "        # so convert them to NumPy arrays.\n",
    "        x = x.numpy()\n",
    "        y = y.numpy()\n",
    "        model, opt_state, train_loss = make_step(model, opt_state, x, y)\n",
    "        if (step % print_every) == 0 or (step == steps - 1):\n",
    "            test_loss, test_accuracy = evaluate(model, testloader)\n",
    "            train_loss, train_accuracy = evaluate(model, trainloader)\n",
    "\n",
    "            writer.add_scalar(\"train loss\", float(train_loss), step)\n",
    "            writer.add_scalar(\"test loss\", float(test_loss), step)\n",
    "\n",
    "            writer.add_scalar(\"train accuracy\", float(train_accuracy), step)\n",
    "            writer.add_scalar(\"test accuracy\", float(test_accuracy), step)\n",
    "\n",
    "            print(\n",
    "                f\"train_loss={train_loss.item()}, train_accuracy={train_accuracy.item()} \"\n",
    "                f\"test_loss={test_loss.item()}, test_accuracy={test_accuracy.item()}\"\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updates =  CNN(\n",
      "  layers=[\n",
      "    Conv2d(\n",
      "      num_spatial_dims=2,\n",
      "      weight=f32[3,1,4,4],\n",
      "      bias=f32[3,1,1],\n",
      "      in_channels=1,\n",
      "      out_channels=3,\n",
      "      kernel_size=(4, 4),\n",
      "      stride=(1, 1),\n",
      "      padding=((0, 0), (0, 0)),\n",
      "      dilation=(1, 1),\n",
      "      groups=1,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    MaxPool2d(\n",
      "      init=None,\n",
      "      operation=None,\n",
      "      num_spatial_dims=2,\n",
      "      kernel_size=(2, 2),\n",
      "      stride=(1, 1),\n",
      "      padding=((0, 0), (0, 0)),\n",
      "      use_ceil=False\n",
      "    ),\n",
      "    None,\n",
      "    None,\n",
      "    Linear(\n",
      "      weight=f32[512,1728],\n",
      "      bias=f32[512],\n",
      "      in_features=1728,\n",
      "      out_features=512,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    None,\n",
      "    Linear(\n",
      "      weight=f32[64,512],\n",
      "      bias=f32[64],\n",
      "      in_features=512,\n",
      "      out_features=64,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    None,\n",
      "    Linear(\n",
      "      weight=f32[10,64],\n",
      "      bias=f32[10],\n",
      "      in_features=64,\n",
      "      out_features=10,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    None\n",
      "  ]\n",
      ")\n",
      "opt_state = (ScaleByAdamState(count=Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=1/0)>, mu=CNN(\n",
      "  layers=[\n",
      "    Conv2d(\n",
      "      num_spatial_dims=2,\n",
      "      weight=f32[3,1,4,4],\n",
      "      bias=f32[3,1,1],\n",
      "      in_channels=1,\n",
      "      out_channels=3,\n",
      "      kernel_size=(4, 4),\n",
      "      stride=(1, 1),\n",
      "      padding=((0, 0), (0, 0)),\n",
      "      dilation=(1, 1),\n",
      "      groups=1,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    MaxPool2d(\n",
      "      init=None,\n",
      "      operation=None,\n",
      "      num_spatial_dims=2,\n",
      "      kernel_size=(2, 2),\n",
      "      stride=(1, 1),\n",
      "      padding=((0, 0), (0, 0)),\n",
      "      use_ceil=False\n",
      "    ),\n",
      "    None,\n",
      "    None,\n",
      "    Linear(\n",
      "      weight=f32[512,1728],\n",
      "      bias=f32[512],\n",
      "      in_features=1728,\n",
      "      out_features=512,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    None,\n",
      "    Linear(\n",
      "      weight=f32[64,512],\n",
      "      bias=f32[64],\n",
      "      in_features=512,\n",
      "      out_features=64,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    None,\n",
      "    Linear(\n",
      "      weight=f32[10,64],\n",
      "      bias=f32[10],\n",
      "      in_features=64,\n",
      "      out_features=10,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    None\n",
      "  ]\n",
      "), nu=CNN(\n",
      "  layers=[\n",
      "    Conv2d(\n",
      "      num_spatial_dims=2,\n",
      "      weight=f32[3,1,4,4],\n",
      "      bias=f32[3,1,1],\n",
      "      in_channels=1,\n",
      "      out_channels=3,\n",
      "      kernel_size=(4, 4),\n",
      "      stride=(1, 1),\n",
      "      padding=((0, 0), (0, 0)),\n",
      "      dilation=(1, 1),\n",
      "      groups=1,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    MaxPool2d(\n",
      "      init=None,\n",
      "      operation=None,\n",
      "      num_spatial_dims=2,\n",
      "      kernel_size=(2, 2),\n",
      "      stride=(1, 1),\n",
      "      padding=((0, 0), (0, 0)),\n",
      "      use_ceil=False\n",
      "    ),\n",
      "    None,\n",
      "    None,\n",
      "    Linear(\n",
      "      weight=f32[512,1728],\n",
      "      bias=f32[512],\n",
      "      in_features=1728,\n",
      "      out_features=512,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    None,\n",
      "    Linear(\n",
      "      weight=f32[64,512],\n",
      "      bias=f32[64],\n",
      "      in_features=512,\n",
      "      out_features=64,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    None,\n",
      "    Linear(\n",
      "      weight=f32[10,64],\n",
      "      bias=f32[10],\n",
      "      in_features=64,\n",
      "      out_features=10,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    None\n",
      "  ]\n",
      ")), EmptyState(), EmptyState())\n",
      "train_loss=0.16880880296230316, train_accuracy=0.9489272236824036 test_loss=0.16895070672035217, test_accuracy=0.9478503465652466\n",
      "train_loss=0.14728350937366486, train_accuracy=0.9576225876808167 test_loss=0.1370496302843094, test_accuracy=0.9609872698783875\n",
      "train_loss=0.12430182099342346, train_accuracy=0.9630863666534424 test_loss=0.11697746068239212, test_accuracy=0.9635748267173767\n",
      "train_loss=0.12771672010421753, train_accuracy=0.9611374139785767 test_loss=0.13335461914539337, test_accuracy=0.9619824886322021\n",
      "train_loss=0.12342427670955658, train_accuracy=0.9615038633346558 test_loss=0.1212184801697731, test_accuracy=0.9612858295440674\n",
      "train_loss=0.10466604679822922, train_accuracy=0.9692830443382263 test_loss=0.104479119181633, test_accuracy=0.9684514403343201\n",
      "train_loss=0.10413675010204315, train_accuracy=0.9683835506439209 test_loss=0.1034366562962532, test_accuracy=0.968949019908905\n",
      "train_loss=0.10076454281806946, train_accuracy=0.9697661399841309 test_loss=0.10659102350473404, test_accuracy=0.9680533409118652\n",
      "train_loss=0.09772640466690063, train_accuracy=0.9696494936943054 test_loss=0.0987643152475357, test_accuracy=0.9664610028266907\n",
      "train_loss=0.09490498155355453, train_accuracy=0.9711986780166626 test_loss=0.09249384701251984, test_accuracy=0.9715366363525391\n",
      "train_loss=0.10641580075025558, train_accuracy=0.968316912651062 test_loss=0.1035940945148468, test_accuracy=0.9684514403343201\n"
     ]
    }
   ],
   "source": [
    "optim = optax.adamw(LEARNING_RATE)\n",
    "\n",
    "# optax.adam(LEARNING_RATE).update()\n",
    "\n",
    "model = train(model, trainloader, testloader, optim, STEPS, PRINT_EVERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'int' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moptax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43me\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optax/_src/alias.py:421\u001b[0m, in \u001b[0;36madam\u001b[0;34m(learning_rate, b1, b2, eps, eps_root, mu_dtype, nesterov)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madam\u001b[39m(\n\u001b[1;32m    308\u001b[0m     learning_rate: base\u001b[38;5;241m.\u001b[39mScalarOrSchedule,\n\u001b[1;32m    309\u001b[0m     b1: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m     nesterov: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    316\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m base\u001b[38;5;241m.\u001b[39mGradientTransformation:\n\u001b[1;32m    317\u001b[0m   \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The Adam optimizer.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03m  Adam is an SGD variant with gradient scaling adaptation. The scaling\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;124;03m  .. seealso:: :func:`optax.nadam`, :func:`optax.adamw`.\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m    412\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m combine\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    413\u001b[0m       transform\u001b[38;5;241m.\u001b[39mscale_by_adam(\n\u001b[1;32m    414\u001b[0m           b1\u001b[38;5;241m=\u001b[39mb1,\n\u001b[1;32m    415\u001b[0m           b2\u001b[38;5;241m=\u001b[39mb2,\n\u001b[1;32m    416\u001b[0m           eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    417\u001b[0m           eps_root\u001b[38;5;241m=\u001b[39meps_root,\n\u001b[1;32m    418\u001b[0m           mu_dtype\u001b[38;5;241m=\u001b[39mmu_dtype,\n\u001b[1;32m    419\u001b[0m           nesterov\u001b[38;5;241m=\u001b[39mnesterov,\n\u001b[1;32m    420\u001b[0m       ),\n\u001b[0;32m--> 421\u001b[0m       \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_by_learning_rate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    422\u001b[0m   )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optax/_src/transform.py:955\u001b[0m, in \u001b[0;36mscale_by_learning_rate\u001b[0;34m(learning_rate, flip_sign)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(learning_rate):\n\u001b[1;32m    954\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m scale_by_schedule(\u001b[38;5;28;01mlambda\u001b[39;00m count: m \u001b[38;5;241m*\u001b[39m learning_rate(count))\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scale(\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'int' and 'dict'"
     ]
    }
   ],
   "source": [
    "optax.adam({\"e\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 3.3234400e-01  4.3591189e-01  1.6111216e-01  3.9150640e-01]\n",
      "   [ 1.0801359e-01 -2.9369362e-03 -8.8275865e-02  3.7083393e-01]\n",
      "   [-1.6904424e-01 -2.5167051e-01 -1.6658500e-01 -2.6474583e-01]\n",
      "   [-1.8858138e-01  8.7269619e-02 -2.4614206e-01  1.7511271e-01]]]\n",
      "\n",
      "\n",
      " [[[ 3.8009986e-02 -2.0937578e-01  1.5259872e-01  8.7809712e-02]\n",
      "   [-1.5437129e-01 -2.5056186e-01 -1.7233023e-01 -2.4398933e-01]\n",
      "   [ 2.2501110e-01  2.4705268e-01 -1.5951101e-01 -2.5225306e-01]\n",
      "   [-2.2187206e-04  3.9700784e-02  2.0728935e-01  2.4530593e-01]]]\n",
      "\n",
      "\n",
      " [[[-9.0941712e-02  1.7508501e-01  1.8408975e-01  1.9494964e-01]\n",
      "   [-3.4508991e-01  6.3339532e-03  3.6781996e-01 -1.1202988e-01]\n",
      "   [-3.3668917e-01 -1.1023605e-02  3.3684352e-01 -4.7630627e-02]\n",
      "   [-1.3542780e-01  1.8202873e-01  3.3244020e-01  7.8686327e-02]]]]\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archive\n",
    "\n",
    "- 0.38 loss - 3.5 min\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "STEPS = 300\n",
    "\n",
    "PRINT_EVERY = 30\n",
    "\n",
    "SEED = 5678\n",
    "\n",
    "\n",
    "- fucked\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "LEARNING_RATE = 3e-2\n",
    "\n",
    "STEPS = 300\n",
    "\n",
    "PRINT_EVERY = 30\n",
    "\n",
    "SEED = 5678\n",
    "\n",
    "\n",
    "- 0.38 loss\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "LEARNING_RATE = 2*3e-4\n",
    "\n",
    "STEPS = 300\n",
    "\n",
    "PRINT_EVERY = 30\n",
    "\n",
    "SEED = 5678\n",
    "\n",
    "\n",
    "- 0.15 loss\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "LEARNING_RATE = 0.005\n",
    "\n",
    "STEPS = 300\n",
    "\n",
    "PRINT_EVERY = 30\n",
    "\n",
    "SEED = 5678"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
