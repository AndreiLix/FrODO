{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same thing, but torch manual_seed instead of torch generators \n",
    "-> Perfectly reproducible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/andrei/Desktop/PROJECT_ELLIS_COMDO/FOLDER_code\n",
      "global train_loss=2.305303692817688, gloabal train_accuracy=0.09670983627438545 global test_loss=2.3039047718048096, global test_accuracy=0.09661788120865822\n",
      "global train_loss=2.1893906593322754, gloabal train_accuracy=0.2486577033996582 global test_loss=2.1851030588150024, global test_accuracy=0.2434731051325798\n",
      "global train_loss=2.080592393875122, gloabal train_accuracy=0.5451133847236633 global test_loss=2.0726053714752197, global test_accuracy=0.552808552980423\n",
      "global train_loss=1.9616005420684814, gloabal train_accuracy=0.5400357097387314 global test_loss=1.950952410697937, global test_accuracy=0.5515229254961014\n",
      "global train_loss=1.8147075772285461, gloabal train_accuracy=0.6582254469394684 global test_loss=1.803224503993988, global test_accuracy=0.6759295761585236\n",
      "global train_loss=1.6652057766914368, gloabal train_accuracy=0.6330743432044983 global test_loss=1.6486510634422302, global test_accuracy=0.6481408178806305\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 385\u001b[0m\n\u001b[1;32m    377\u001b[0m     trainloader_Agent2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m    378\u001b[0m         Agent2_Train_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    379\u001b[0m     )\n\u001b[1;32m    380\u001b[0m     testloader_Agent2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m    381\u001b[0m         Agent2_Test_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    382\u001b[0m     )\n\u001b[0;32m--> 385\u001b[0m     trained_models \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer_fractional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader_Agent1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrainloader_Agent1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader_Agent2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrainloader_Agent2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader_Agent1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtestloader_Agent1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader_Agent2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtestloader_Agent2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSTEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPRINT_EVERY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# TODO change results path\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/u699081/FOLDER_comdo/SIMULATIONS/RESULTS/ANNs/Fractional/test_acc.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn [1], line 254\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(optimizer, models, trainloader_Agent1, trainloader_Agent2, testloader_Agent1, testloader_Agent2, steps, print_every)\u001b[0m\n\u001b[1;32m    252\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m  \u001b[38;5;28mfloat\u001b[39m(evaluate(models[\u001b[38;5;241m0\u001b[39m], trainloader_Agent1)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    253\u001b[0m train_accuracy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(evaluate(models[\u001b[38;5;241m0\u001b[39m], trainloader_Agent1)[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 254\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader_Agent2\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    255\u001b[0m train_accuracy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(evaluate(models[\u001b[38;5;241m1\u001b[39m], trainloader_Agent2)[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# print(\"Seconds to compute loss and accuracy: \", time.time() - start)\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# start = time.time()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [1], line 99\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, testloader)\u001b[0m\n\u001b[1;32m     97\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     98\u001b[0m avg_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m testloader:\n\u001b[1;32m    100\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    101\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "# os.chdir(\"/home/u699081/FOLDER_comdo\")\n",
    "os.chdir(\"/home/andrei/Desktop/PROJECT_ELLIS_COMDO/FOLDER_code\")\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "# import optax  # https://github.com/deepmind/optax\n",
    "import torch  # https://pytorch.org\n",
    "import torchvision  # https://pytorch.org\n",
    "from jaxtyping import Array, Float, Int, PyTree  # https://github.com/google/jaxtyping\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "import time\n",
    "import comdo\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "# _________________________ Utils \n",
    "\n",
    "class CNN(eqx.Module):\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, key):\n",
    "        key1, key2, key3, key4 = jax.random.split(key, 4)\n",
    "        # Standard CNN setup: convolutional layer, followed by flattening,\n",
    "        # with a small MLP on top.\n",
    "        self.layers = [\n",
    "            eqx.nn.Conv2d(1, 3, kernel_size=4, key=key1),\n",
    "            eqx.nn.MaxPool2d(kernel_size=2),\n",
    "            jax.nn.relu,\n",
    "            jnp.ravel,\n",
    "            eqx.nn.Linear(1728, 512, key=key2),\n",
    "            jax.nn.sigmoid,\n",
    "            eqx.nn.Linear(512, 64, key=key3),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.Linear(64, 10, key=key4),\n",
    "            jax.nn.log_softmax,\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x: Float[Array, \"1 28 28\"]) -> Float[Array, \"10\"]:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss(\n",
    "    model: CNN, x: Float[Array, \"batch 1 28 28\"], y: Int[Array, \" batch\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    # Our input has the shape (BATCH_SIZE, 1, 28, 28), but our model operations on\n",
    "    # a single input input image of shape (1, 28, 28).\n",
    "    #\n",
    "    # Therefore, we have to use jax.vmap, which in this case maps our model over the\n",
    "    # leading (batch) axis.\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    return cross_entropy(y, pred_y)\n",
    "\n",
    "\n",
    "def cross_entropy(\n",
    "    y: Int[Array, \" batch\"], pred_y: Float[Array, \"batch 10\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    # y are the true targets, and should be integers 0-9.\n",
    "    # pred_y are the log-softmax'd predictions.\n",
    "    pred_y = jnp.take_along_axis(pred_y, jnp.expand_dims(y, 1), axis=1)\n",
    "    return -jnp.mean(pred_y)\n",
    "\n",
    "\n",
    "loss = eqx.filter_jit(loss)  # JIT our loss function from earlier!\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def compute_accuracy(\n",
    "    model: CNN, x: Float[Array, \"batch 1 28 28\"], y: Int[Array, \" batch\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    \"\"\"This function takes as input the current model\n",
    "    and computes the average accuracy on a batch.\n",
    "    \"\"\"\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    pred_y = jnp.argmax(pred_y, axis=1)\n",
    "    return jnp.mean(y == pred_y)\n",
    "\n",
    "\n",
    "def evaluate(model: CNN, testloader: torch.utils.data.DataLoader):\n",
    "    \"\"\"This function evaluates the model on the test dataset,\n",
    "    computing both the average loss and the average accuracy.\n",
    "    \"\"\"\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    for x, y in testloader:\n",
    "        x = x.numpy()\n",
    "        y = y.numpy()\n",
    "        # Note that all the JAX operations happen inside `loss` and `compute_accuracy`,\n",
    "        # and both have JIT wrappers, so this is fast.\n",
    "        avg_loss += loss(model, x, y)\n",
    "        avg_acc += compute_accuracy(model, x, y)\n",
    "    return avg_loss / len(testloader), avg_acc / len(testloader)\n",
    "\n",
    "\n",
    "# _________________________________________________________________________\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "# TODO: choose nr. of steps (default = 600)\n",
    "STEPS = 1800\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "LEARNING_RATE = 0.005\n",
    "\n",
    "PRINT_EVERY = 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ________________________ train script __________________-\n",
    "\n",
    "from comdo.utils_ANNs import DOptimizer\n",
    "\n",
    "def train(\n",
    "    optimizer,\n",
    "    models: CNN,\n",
    "    trainloader_Agent1: torch.utils.data.DataLoader,\n",
    "    trainloader_Agent2: torch.utils.data.DataLoader,\n",
    "    testloader_Agent1: torch.utils.data.DataLoader,\n",
    "    testloader_Agent2: torch.utils.data.DataLoader,\n",
    "    steps: int,\n",
    "    print_every: int,\n",
    ") -> CNN:\n",
    "    # Just like earlier: It only makes sense to train the arrays in our model,\n",
    "    # so filter out everything else.\n",
    "    # opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "\n",
    "\n",
    "    # Always wrap everything -- computing gradients, running the optimiser, updating\n",
    "    # the model -- into a single JIT region. This ensures things run as fast as\n",
    "    # possible.\n",
    "\n",
    "\n",
    "    # @eqx.filter_jit\n",
    "    def make_step(\n",
    "        models: CNN,\n",
    "        x_agent1: Float[Array, \"batch 1 28 28\"],\n",
    "        y_agent1: Int[Array, \" batch\"],\n",
    "        x_agent2: Float[Array, \"batch 1 28 28\"],\n",
    "        y_agent2: Int[Array, \" batch\"],\n",
    "\n",
    "    ):\n",
    "\n",
    "        # making list with the grads (pytrees) of th etwo agents\n",
    "        grads_list = []\n",
    "        grads_list.append(eqx.filter_value_and_grad(loss)(models[0], x_agent1, y_agent1)[1])\n",
    "        grads_list.append(eqx.filter_value_and_grad(loss)(models[1], x_agent2, y_agent2)[1])\n",
    "\n",
    "        # loss_value, grads = eqx.filter_value_and_grad(loss)(model, x, y)\n",
    "        # updates, opt_state = optim.update(grads, opt_state, model)\n",
    "\n",
    "        # model = eqx.apply_updates(model, updates)\n",
    "        \n",
    "        models = optimizer.step_withMemory(models, grads_list)\n",
    "\n",
    "        return models\n",
    "    \n",
    "    # Loop over our training dataset as many times as we need.\n",
    "    def infinite_trainloader_agent1():\n",
    "        while True:\n",
    "            yield from trainloader_Agent1\n",
    "\n",
    "    def infinite_trainloader_agent2():\n",
    "        while True: \n",
    "            yield from trainloader_Agent2\n",
    "\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    list_test_accuracy = []\n",
    "    list_test_loss =[]\n",
    "    list_train_accuracy = []\n",
    "    list_train_loss =[]\n",
    "\n",
    "    for step, (x_agent1, y_agent1), (x_agent2, y_agent2) in zip(range(steps), infinite_trainloader_agent1(), infinite_trainloader_agent2()):\n",
    "        # PyTorch dataloaders give PyTorch tensors by default,\n",
    "        # so convert them to NumPy arrays.\n",
    "        x_agent1 = x_agent1.numpy()\n",
    "        y_agent1 = y_agent1.numpy()\n",
    "\n",
    "        x_agent2 = x_agent2.numpy()\n",
    "        y_agent2 = y_agent2.numpy()\n",
    "        \n",
    "        # start = time.time()\n",
    "        \n",
    "\n",
    "        # initial allignent, in case of initial difference in states between agents\n",
    "\n",
    "        if step == 0:\n",
    "            for layer_i in optimizer.idx_layersWithWeights:\n",
    "\n",
    "                    aux_update = (1/optimizer.n_agents) * sum( models[agent_j].layers[layer_i].weight for agent_j in range(optimizer.n_agents) )\n",
    "                    \n",
    "                    for agent_i in range(optimizer.n_agents):\n",
    "                        where = lambda m: m[agent_i].layers[layer_i].weight\n",
    "                        models = eqx.tree_at(where, models, aux_update)\n",
    "\n",
    "\n",
    "        models = make_step(models, x_agent1, y_agent1, x_agent2, y_agent2)\n",
    "            \n",
    "        # print(\"Made step \", step )\n",
    "        # print(\"Seconds to take this step: \", time.time() - start)\n",
    "\n",
    "        # print(\"Z_g\")\n",
    "        # print(optimizer.z_g)\n",
    "\n",
    "        if (step % print_every) == 0 or (step == steps - 1):\n",
    "\n",
    "            test_loss = 0\n",
    "            test_accuracy = 0\n",
    "            train_loss = 0\n",
    "            train_accuracy = 0\n",
    "\n",
    "            # start = time.time()\n",
    "\n",
    "            test_loss +=  float(evaluate(models[0], testloader_Agent1)[0])\n",
    "            test_accuracy +=  float(evaluate(models[0], testloader_Agent1)[1])\n",
    "            \n",
    "            test_loss += float(evaluate(models[1], testloader_Agent2)[0])\n",
    "            test_accuracy += float(evaluate(models[1], testloader_Agent2)[1])\n",
    "\n",
    "            train_loss +=  float(evaluate(models[0], trainloader_Agent1)[0])\n",
    "            train_accuracy += float(evaluate(models[0], trainloader_Agent1)[1])\n",
    "            train_loss += float(evaluate(models[1], trainloader_Agent2)[0])\n",
    "            train_accuracy += float(evaluate(models[1], trainloader_Agent2)[1])\n",
    "\n",
    "\n",
    "            # print(\"Seconds to compute loss and accuracy: \", time.time() - start)\n",
    "\n",
    "            # start = time.time()\n",
    "\n",
    "            writer.add_scalar(\"global train loss\", train_loss/2, step)   # printing means\n",
    "            writer.add_scalar(\"global test loss\", test_loss/2, step)\n",
    "\n",
    "            writer.add_scalar(\"global train accuracy\", train_accuracy/2, step)\n",
    "            writer.add_scalar(\"global test accuracy\", test_accuracy/2, step)\n",
    "\n",
    "            # ______ appending performance to lists ______________-\n",
    "\n",
    "            list_test_accuracy.append(test_accuracy/2) \n",
    "            list_test_loss.append(test_loss/2)\n",
    "            list_train_accuracy.append(train_accuracy/2) \n",
    "            list_train_loss.append(train_loss/2)\n",
    "\n",
    "            # if (step % print_every) == 0 or (step == steps - 1):\n",
    "            print(\n",
    "                f\"global train_loss={train_loss/2}, gloabal train_accuracy={train_accuracy/2} \"\n",
    "                f\"global test_loss={test_loss/2}, global test_accuracy={test_accuracy/2}\" )\n",
    "            \n",
    "            # print(\"Seconds to write and print: \", time.time() - start)\n",
    "\n",
    "    list_test_Accuracies_acrossInitializations.append(list_test_accuracy)\n",
    "    list_test_Loss_acrossInitializations.append(list_test_loss)\n",
    "    list_train_Accuracies_acrossInitializations.append(list_train_accuracy)\n",
    "    list_train_Loss_acrossInitializations.append(list_train_loss)\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "\n",
    "# TODO: fill in range with the number of runs you want\n",
    "SEEDS = range(5)\n",
    "\n",
    "\n",
    "\n",
    "list_test_Accuracies_acrossInitializations = []\n",
    "list_test_Loss_acrossInitializations = []\n",
    "list_train_Accuracies_acrossInitializations = []\n",
    "list_train_Loss_acrossInitializations = []\n",
    "\n",
    "for SEED in SEEDS:\n",
    "\n",
    "    key = jax.random.PRNGKey(SEED)\n",
    "    key, subkey = jax.random.split(key, 2)\n",
    "\n",
    "    agent1 = CNN(subkey)\n",
    "    agent2 = CNN(subkey+100)\n",
    "\n",
    "    models = [agent1, agent2]\n",
    "\n",
    "    optimizer_fractional = comdo.utils_ANNs.DOptimizer(models= models,\n",
    "                                            beta_c = 1,\n",
    "                                            beta_g = LEARNING_RATE,\n",
    "                                            beta_gm = LEARNING_RATE / 2)                                        \n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    normalise_data = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        \"MNIST\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=normalise_data,\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        \"MNIST\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=normalise_data,\n",
    "    )\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    "\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    "    )\n",
    "\n",
    "    # ____________________ splitting MNIST into 2 distinct datasets __________________\n",
    "    # import os\n",
    "    # os.chdir(\"/home/andrei/Desktop/PROJECT_ELLIS_COMDO/FOLDER_code\")\n",
    "\n",
    "    from comdo.utils_ANNs import get_2DO_datasets\n",
    "\n",
    "\n",
    "    Agent1_Train_dataset, Agent1_Test_dataset, Agent2_Train_dataset, Agent2_Test_dataset = \\\n",
    "        get_2DO_datasets(train_dataset= train_dataset, test_dataset= test_dataset, seed= SEED)\n",
    "\n",
    "\n",
    "    trainloader_Agent1 = torch.utils.data.DataLoader(\n",
    "        Agent1_Train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    "    )\n",
    "    testloader_Agent1 = torch.utils.data.DataLoader(\n",
    "        Agent1_Test_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    "    )\n",
    "\n",
    "    trainloader_Agent2 = torch.utils.data.DataLoader(\n",
    "        Agent2_Train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    "    )\n",
    "    testloader_Agent2 = torch.utils.data.DataLoader(\n",
    "        Agent2_Test_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    "    )\n",
    "\n",
    "\n",
    "    trained_models = train(optimizer= optimizer_fractional, models= models, trainloader_Agent1= trainloader_Agent1, trainloader_Agent2= trainloader_Agent2, testloader_Agent1= testloader_Agent1, testloader_Agent2= testloader_Agent2, steps= STEPS, print_every= PRINT_EVERY)\n",
    "\n",
    "\n",
    "# TODO change results path\n",
    "\n",
    "with open('/home/u699081/FOLDER_comdo/SIMULATIONS/RESULTS/ANNs/Fractional/test_acc.pkl', 'wb') as f:\n",
    "    pickle.dump(list_test_Accuracies_acrossInitializations, f)\n",
    "\n",
    "with open('/home/u699081/FOLDER_comdo/SIMULATIONS/RESULTS/ANNs/Fractional/test_loss.pkl', 'wb') as f:\n",
    "    pickle.dump(list_test_Loss_acrossInitializations, f)\n",
    "\n",
    "with open('/home/u699081/FOLDER_comdo/SIMULATIONS/RESULTS/ANNs/Fractional/train_acc.pkl', 'wb') as f:\n",
    "    pickle.dump(list_train_Accuracies_acrossInitializations, f)\n",
    "\n",
    "with open('/home/u699081/FOLDER_comdo/SIMULATIONS/RESULTS/ANNs/Fractional/train_loss.pkl', 'wb') as f:\n",
    "    pickle.dump(list_train_Loss_acrossInitializations, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
